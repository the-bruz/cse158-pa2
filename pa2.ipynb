{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gzip\n",
    "from collections import defaultdict\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import random\n",
    "from sklearn import linear_model\n",
    "import string\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.corpus import stopwords\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def readGz(path):\n",
    "    for l in gzip.open(path, 'rt'):\n",
    "        yield eval(l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = []\n",
    "count = 0\n",
    "for d in readGz('ratebeer.json.gz'):\n",
    "    count += 1\n",
    "    data.append(d)\n",
    "    d['review/overall'] = int(d['review/overall'].split('/')[0])\n",
    "    if (count >= 100000):\n",
    "        break\n",
    "random.seed(30)\n",
    "random.shuffle(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainSet = data[:80000]\n",
    "validSet = data[80000:90000]\n",
    "testSet = data[90000:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Baseline Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "allRatings = []\n",
    "for d in trainSet:\n",
    "    allRatings.append(int(d['review/overall']))\n",
    "avgRating = sum(allRatings) / len(allRatings)\n",
    "print(avgRating)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "basePred = [avgRating] * len(testSet)\n",
    "testRatings = [d['review/overall'] for d in testSet]\n",
    "validRatings = [d['review/overall'] for d in validSet]\n",
    "print(mean_squared_error(basePred, testRatings), mean_squared_error(basePred, validRatings))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Jaccard Similarity Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "itemPerUser = defaultdict(list)\n",
    "userPerItem = defaultdict(list)\n",
    "brewerSet = set()\n",
    "for d in trainSet:\n",
    "    itemPerUser[d['review/profileName']].append((d['beer/beerId'], d['review/overall']))\n",
    "    userPerItem[d['beer/beerId']].append((d['review/profileName'], d['review/overall']))\n",
    "    brewerSet.add(d['beer/brewerId'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def jaccard(s1, s2):\n",
    "    numer = len(s1.intersection(s2))\n",
    "    denom = len(s1.union(s2))\n",
    "    return numer / denom"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predictByItem(user, item):\n",
    "    sims = []\n",
    "    ratings = []\n",
    "    for j in itemPerUser[user]:\n",
    "        if (j[0] == item):\n",
    "            continue\n",
    "        users1 = set(userPerItem[item])\n",
    "        users2 = set(userPerItem[j[0]])\n",
    "        sims.append(jaccard(users1, users2))\n",
    "        ratings.append(j[1])\n",
    "    z = sum(sims)\n",
    "    if (z == 0): return avgRating\n",
    "    return 1 / z * sum([s * r for s, r in zip(sims, ratings)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "jacItemPred = []\n",
    "for d in validSet:\n",
    "    jacItemPred.append(predictByItem(d['review/profileName'], d['beer/beerId']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(mean_squared_error(jacItemPred, validRatings))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predictByUser(user, item):\n",
    "    sims = []\n",
    "    ratings = []\n",
    "    for v in userPerItem[item]:\n",
    "        if (v[0] == user):\n",
    "            continue\n",
    "        items1 = set(itemPerUser[user])\n",
    "        items2 = set(itemPerUser[v[0]])\n",
    "        sims.append(jaccard(items1, items2))\n",
    "        ratings.append(v[1])\n",
    "    z = sum(sims)\n",
    "    if (z == 0): return avgRating\n",
    "    return 1 / z * sum([s * r for s, r in zip(sims, ratings)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "jacItemPred = []\n",
    "for d in validSet:\n",
    "    jacItemPred.append(predictByUser(d['review/profileName'], d['beer/beerId']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(mean_squared_error(jacItemPred, validRatings))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predictByUserV2(user, item):\n",
    "    sims = []\n",
    "    ratings = []\n",
    "    for v in userPerItem[item]:\n",
    "        if (v[0] == user):\n",
    "            continue\n",
    "        items1 = set(itemPerUser[user])\n",
    "        items2 = set(itemPerUser[v[0]])\n",
    "        sims.append(jaccard(items1, items2))\n",
    "        ratings_v = [i[1] for i in itemPerUser[v[0]]]\n",
    "        ratings.append(v[1] - (sum(ratings_v) / len(ratings_v)))\n",
    "    z = sum(sims)\n",
    "    if (z == 0): return avgRating\n",
    "    ratings_u = [i[1] for i in itemPerUser[user]]\n",
    "    return (sum(ratings_u) / len(ratings_u)) + (1 / z) * sum([s * r for s, r in zip(sims, ratings)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "jacItemPred = []\n",
    "for d in validSet:\n",
    "    jacItemPred.append(predictByUserV2(d['review/profileName'], d['beer/beerId']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(mean_squared_error(jacItemPred, validRatings))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predictByUserV3(user, item):\n",
    "    sims = []\n",
    "    ratings = []\n",
    "    for v in userPerItem[item]:\n",
    "        if (v[0] == user):\n",
    "            continue\n",
    "        items1 = set(itemPerUser[user])\n",
    "        items2 = set(itemPerUser[v[0]])\n",
    "        sims.append(jaccard(items1, items2))\n",
    "        ratings.append(v[1])\n",
    "    z = sum(sims)\n",
    "    if (z == 0):\n",
    "        if (len(userPerItem[item]) != 0):\n",
    "            ratings_i = [i[1] for i in userPerItem[item]]\n",
    "            return sum(ratings_i) / len(ratings_i)\n",
    "        else:\n",
    "            return avgRating\n",
    "    else:\n",
    "        return 1 / z * sum([s * r for s, r in zip(sims, ratings)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "jacItemPred = []\n",
    "for d in validSet:\n",
    "    jacItemPred.append(predictByUserV3(d['review/profileName'], d['beer/beerId']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(mean_squared_error(jacItemPred, validRatings))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "jacItemPred = []\n",
    "for d in testSet:\n",
    "    count += 1\n",
    "    jacItemPred.append(predictByUserV3(d['review/profileName'], d['beer/beerId']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(mean_squared_error(jacItemPred, testRatings))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bag of Words Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleanText (review):\n",
    "    words = review.strip().split()\n",
    "    return words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for d in trainSet:\n",
    "    words = cleanText(d['review/text'])\n",
    "    d['words'] = words\n",
    "for d in validSet:\n",
    "    words = cleanText(d['review/text'])\n",
    "    d['words'] = words\n",
    "for d in testSet:\n",
    "    words = cleanText(d['review/text'])\n",
    "    d['words'] = words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wordDic = defaultdict(int)\n",
    "for d in trainSet:\n",
    "    for word in d['words']:\n",
    "        wordDic[word] += 1\n",
    "\n",
    "counts = []\n",
    "for key, value in wordDic.items():\n",
    "    counts.append((value, key))\n",
    "counts.sort(key = lambda x: x[0], reverse = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dicSize = 2000\n",
    "subCounts = counts[:dicSize]\n",
    "wordID = {}\n",
    "count = 0\n",
    "wordSet = set()\n",
    "for freq, word in subCounts:\n",
    "    wordSet.add(word)\n",
    "    wordID[word] = count\n",
    "    count += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature(words):\n",
    "    f = [0.0] * dicSize\n",
    "    for word in words:\n",
    "        if (word in wordSet):\n",
    "            f[wordID[word]] += 1.0\n",
    "    f.append(1.0)\n",
    "    return f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Xtrain = [feature(d['words']) for d in trainSet]\n",
    "ytrain = [d['review/overall'] for d in trainSet]\n",
    "Xvalid = [feature(d['words']) for d in validSet]\n",
    "yvalid = [d['review/overall'] for d in validSet]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mod = linear_model.LinearRegression(n_jobs=-1)\n",
    "mod.fit(Xtrain, ytrain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ypred = mod.predict(Xvalid)\n",
    "print(mean_squared_error(yvalid, ypred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleanTextV2 (review):\n",
    "    txt = ''.join([c.lower() for c in review if c not in string.punctuation])\n",
    "    words = txt.strip().split()\n",
    "    return words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for d in trainSet:\n",
    "    words = cleanTextV2(d['review/text'])\n",
    "    d['words'] = words\n",
    "for d in validSet:\n",
    "    words = cleanTextV2(d['review/text'])\n",
    "    d['words'] = words\n",
    "for d in testSet:\n",
    "    words = cleanTextV2(d['review/text'])\n",
    "    d['words'] = words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wordDic = defaultdict(int)\n",
    "for d in trainSet:\n",
    "    for word in d['words']:\n",
    "        wordDic[word] += 1\n",
    "\n",
    "counts = []\n",
    "for key, value in wordDic.items():\n",
    "    counts.append((value, key))\n",
    "counts.sort(key = lambda x: x[0], reverse = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dicSize = 2000\n",
    "subCounts = counts[:dicSize]\n",
    "wordID = {}\n",
    "count = 0\n",
    "wordSet = set()\n",
    "for freq, word in subCounts:\n",
    "    wordSet.add(word)\n",
    "    wordID[word] = count\n",
    "    count += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Xtrain = [feature(d['words']) for d in trainSet]\n",
    "ytrain = [d['review/overall'] for d in trainSet]\n",
    "Xvalid = [feature(d['words']) for d in validSet]\n",
    "yvalid = [d['review/overall'] for d in validSet]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mod = linear_model.LinearRegression(n_jobs=-1)\n",
    "mod.fit(Xtrain, ytrain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ypred = mod.predict(Xvalid)\n",
    "print(mean_squared_error(yvalid, ypred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# best model starts here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = set(stopwords.words('english'))\n",
    "stemmer = PorterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleanTextV3 (review):\n",
    "    txt = ''.join([c.lower() for c in review if c not in string.punctuation])\n",
    "    words = txt.strip().split()\n",
    "    words = [stemmer.stem(word) for word in words if word not in stop_words]\n",
    "    return words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "for d in trainSet:\n",
    "    words = cleanTextV3(d['review/text'])\n",
    "    d['words'] = words\n",
    "for d in validSet:\n",
    "    words = cleanTextV3(d['review/text'])\n",
    "    d['words'] = words\n",
    "for d in testSet:\n",
    "    words = cleanTextV3(d['review/text'])\n",
    "    d['words'] = words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "wordDic = defaultdict(int)\n",
    "docCount = defaultdict(int)\n",
    "for d in trainSet:\n",
    "    for word in d['words']:\n",
    "        wordDic[word] += 1\n",
    "    for w in set(d['words']):\n",
    "        docCount[w] += 1\n",
    "\n",
    "counts = []\n",
    "for key, value in wordDic.items():\n",
    "    counts.append((value, key))\n",
    "counts.sort(key = lambda x: x[0], reverse = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "dicSize = 2000\n",
    "subCounts = counts[:dicSize]\n",
    "wordID = {}\n",
    "count = 0\n",
    "wordSet = set()\n",
    "for freq, word in subCounts:\n",
    "    wordSet.add(word)\n",
    "    wordID[word] = count\n",
    "    count += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "idf = [0] * dicSize\n",
    "for word in wordID:\n",
    "    idf[wordID[word]] = math.log10(len(trainSet) / docCount[word])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature(words):\n",
    "    f = [0.0] * dicSize\n",
    "    indices = set()\n",
    "    for word in words:\n",
    "        if (word in wordSet):\n",
    "            index = wordID[word]\n",
    "            f[index] += 1.0\n",
    "            indices.add(index)\n",
    "    mx = max(f)\n",
    "    if (mx != 0.0):\n",
    "        for i in indices:\n",
    "            f[i] = f[i] / mx * idf[i]\n",
    "    f.append(1.0)\n",
    "    return f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "Xtrain = [feature(d['words']) for d in trainSet]\n",
    "ytrain = [d['review/overall'] for d in trainSet]\n",
    "Xvalid = [feature(d['words']) for d in validSet]\n",
    "yvalid = [d['review/overall'] for d in validSet]\n",
    "Xtest = [feature(d['words']) for d in testSet]\n",
    "ytest = [d['review/overall'] for d in testSet]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "del counts, subCounts, wordDic, wordSet, wordID, docCount, idf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LinearRegression(n_jobs=-1)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mod = linear_model.LinearRegression(n_jobs=-1)\n",
    "mod.fit(Xtrain, ytrain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.866160748056001 4.978496080872587\n"
     ]
    }
   ],
   "source": [
    "ypred = mod.predict(Xvalid)\n",
    "tpred = mod.predict(Xtest)\n",
    "print(mean_squared_error(yvalid, ypred), mean_squared_error(ytest, tpred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del Xtrain, ytrain, Xvalid, yvalid, Xtest, ytest, ypred, tpred"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "bd79deaf55d5c2f6d19df0f2fceb72daaf88a8c784b56804b53b61ff3e27dc69"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
